{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.6.1\n",
      "Available devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (includes compilation time):\n",
      "JIT function first call: 1.789231 seconds\n",
      "Regular function: 0.014970 seconds\n",
      "\n",
      "Second call (compiled version):\n",
      "JIT function second call: 0.001622 seconds\n",
      "\n",
      "Speedup: 9.23x\n"
     ]
    }
   ],
   "source": [
    "def slow_function(x):\n",
    "    \"\"\"A function without JIT compilation\"\"\"\n",
    "    for i in range(100):\n",
    "        x = jnp.sin(x) + jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "@jit\n",
    "def fast_function(x):\n",
    "    \"\"\"The same function with JIT compilation\"\"\"\n",
    "    for i in range(100):\n",
    "        x = jnp.sin(x) + jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# Time the functions\n",
    "print(\"First call (includes compilation time):\")\n",
    "start = time.time()\n",
    "result_fast = fast_function(x)\n",
    "fast_time_first = time.time() - start\n",
    "print(f\"JIT function first call: {fast_time_first:.6f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "result_slow = slow_function(x)\n",
    "slow_time = time.time() - start\n",
    "print(f\"Regular function: {slow_time:.6f} seconds\")\n",
    "\n",
    "print(\"\\nSecond call (compiled version):\")\n",
    "start = time.time()\n",
    "result_fast = fast_function(x)\n",
    "result_fast.block_until_ready()  # Ensure the computation is complete\n",
    "fast_time_second = time.time() - start\n",
    "print(f\"JIT function second call: {fast_time_second:.6f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup: {slow_time / fast_time_second:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call with specific shapes:\n",
      "Traced<float32[2]>with<DynamicJaxprTrace>\n",
      "Result: (Array([4., 6.], dtype=float32), Array([3., 8.], dtype=float32))\n",
      "\n",
      "Second call with same shapes:\n",
      "Result: (Array([12., 14.], dtype=float32), Array([35., 48.], dtype=float32))\n",
      "\n",
      "Call with different shapes (triggers recompilation):\n",
      "Traced<float32[3]>with<DynamicJaxprTrace>\n",
      "Result: (Array([5., 7., 9.], dtype=float32), Array([ 4., 10., 18.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def add_multiply(x, y):\n",
    "    print(x)\n",
    "    return x + y, x * y\n",
    "\n",
    "# First call - triggers compilation\n",
    "print(\"First call with specific shapes:\")\n",
    "a = jnp.array([1.0, 2.0])\n",
    "b = jnp.array([3.0, 4.0])\n",
    "result1 = add_multiply(a, b)\n",
    "print(f\"Result: {result1}\")\n",
    "\n",
    "# Second call with same shapes - uses cached compilation\n",
    "print(\"\\nSecond call with same shapes:\")\n",
    "c = jnp.array([5.0, 6.0])\n",
    "d = jnp.array([7.0, 8.0])\n",
    "result2 = add_multiply(c, d)\n",
    "print(f\"Result: {result2}\")\n",
    "\n",
    "# Different shapes trigger recompilation\n",
    "print(\"\\nCall with different shapes (triggers recompilation):\")\n",
    "e = jnp.array([1.0, 2.0, 3.0])  # Different shape!\n",
    "f = jnp.array([4.0, 5.0, 6.0])\n",
    "result3 = add_multiply(e, f)\n",
    "print(f\"Result: {result3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Static vs Dynamic Arguments in JIT ===\n",
      "\n",
      "❌ Trying dynamic loop bounds:\n",
      "Failed as expected: TracerIntegerConversionError\n",
      "Error message: The __index__() method was called on traced array with shape int32[]\n",
      "The error occurred while tracin...\n",
      "\n",
      "✅ Using static loop bounds:\n",
      "5 iterations: [1.6715611 3.282071  4.8925824]\n",
      "3 iterations: [1.3641001 2.6951    4.0261006]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Static vs Dynamic Arguments in JIT ===\\n\")\n",
    "\n",
    "# Example 1: Loop bounds MUST be static\n",
    "@jit\n",
    "def bad_dynamic_loop(x, num_iterations):\n",
    "    \"\"\"This will fail - loop bounds must be known at compile time\"\"\"\n",
    "    for i in range(num_iterations):  # ERROR: num_iterations must be static!\n",
    "        x = x * 1.1 + 0.01\n",
    "    return x\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def good_static_loop(x, num_iterations):\n",
    "    \"\"\"This works - num_iterations is marked as static\"\"\"\n",
    "    for i in range(num_iterations):\n",
    "        x = x * 1.1 + 0.01\n",
    "    return x\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# This will fail with a TracerIntegerConversionError\n",
    "print(\"❌ Trying dynamic loop bounds:\")\n",
    "try:\n",
    "    result = bad_dynamic_loop(x, 5)\n",
    "    print(f\"Unexpected success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed as expected: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n✅ Using static loop bounds:\")\n",
    "result = good_static_loop(x, 5)\n",
    "print(f\"5 iterations: {result}\")\n",
    "\n",
    "result = good_static_loop(x, 3)  # Different static value = recompilation\n",
    "print(f\"3 iterations: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Control Flow in JIT: Good vs Bad Examples ===\n",
      "\n",
      "❌ JIT failed on bad function: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function bad_python_if at /tmp/ipykernel_836/737641638.py:11 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
      "See https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\n",
      "Traced<float32[3]>with<DynamicJaxprTrace>\n",
      "[ 0.5403023 -0.4161468 -0.9899925]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Control Flow in JIT: Good vs Bad Examples ===\\n\")\n",
    "\n",
    "# ✅ GOOD: Using jnp.where for conditional logic\n",
    "@jit\n",
    "def good_conditional(x, threshold=5.0):\n",
    "    \"\"\"Control flow that works with JIT\"\"\"\n",
    "    print(x)\n",
    "    return jnp.where(x > threshold, jnp.sin(x), jnp.cos(x))\n",
    "\n",
    "# ❌ BAD: Python if/else with array-dependent conditions\n",
    "def bad_python_if(x):\n",
    "    \"\"\"This will cause issues with JIT due to Python control flow\"\"\"\n",
    "    if jnp.sum(x) > 5.0:  # Condition depends on array values\n",
    "        return jnp.sin(x)\n",
    "    else:\n",
    "        return jnp.cos(x)\n",
    "\n",
    "# Let's see what happens when we JIT the bad version\n",
    "try:\n",
    "    bad_jit = jit(bad_python_if)\n",
    "    x_test = jnp.array([1.0, 2.0, 3.0])\n",
    "    result = bad_jit(x_test)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ JIT failed on bad function: {e}\")\n",
    "    \n",
    "print(good_conditional(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loops in JIT: Python vs JAX primitives ===\n",
      "\n",
      "1. Moving Average Computation:\n",
      "Data: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "Python loop result: [2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "Scan result: [2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loops in JIT: Python vs JAX primitives ===\\n\")\n",
    "\n",
    "# Example 1: Moving average with different approaches\n",
    "print(\"1. Moving Average Computation:\")\n",
    "\n",
    "# ❌ Python loop approach (works but not optimal)\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def moving_avg_python_loop(data, window_size):\n",
    "    \"\"\"Moving average using Python loop - requires static window_size\"\"\"\n",
    "    n = len(data)\n",
    "    result = jnp.zeros(n - window_size + 1)\n",
    "    \n",
    "    for i in range(len(result)):  # Loop bound must be static!\n",
    "        result = result.at[i].set(jnp.mean(data[i:i+window_size]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ✅ Using jax.lax.scan (better for dynamic operations)\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def moving_avg_scan(data, window_size):\n",
    "    \"\"\"Moving average using scan - more flexible\"\"\"\n",
    "    def scan_fn(i, _):\n",
    "        avg = jnp.mean(jax.lax.dynamic_slice(data, (i,), (window_size,)))\n",
    "        return i + 1, avg\n",
    "\n",
    "    n_windows = data.shape[0] - window_size + 1\n",
    "    _, averages = jax.lax.scan(scan_fn, 0, xs=None, length=n_windows)\n",
    "    return averages\n",
    "\n",
    "# Test data\n",
    "data = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n",
    "window_size = 3\n",
    "\n",
    "print(\"Data:\", data)\n",
    "print(\"Python loop result:\", moving_avg_python_loop(data, window_size))\n",
    "print(\"Scan result:\", moving_avg_scan(data, window_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 1.6350002\n",
      "Gradients: {'b': Array(-2.3000002, dtype=float32, weak_type=True), 'w': Array(-7.0000005, dtype=float32, weak_type=True)}\n",
      "Batch predictions: [[1.6 3.1]\n",
      " [4.6 6.1]\n",
      " [7.6 9.1]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Combining JIT with Other JAX Transformations\n",
    "@jit  # JIT the gradient computation\n",
    "def loss_function(params, x, y):\n",
    "    \"\"\"A simple quadratic loss function\"\"\"\n",
    "    pred = params['w'] * x + params['b']\n",
    "    return jnp.mean((pred - y)**2)\n",
    "\n",
    "# Create the gradient function and JIT it\n",
    "grad_loss = jit(grad(loss_function, argnums=0))\n",
    "\n",
    "# Sample data\n",
    "x_data = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "y_data = jnp.array([2.0, 4.0, 6.0, 8.0])  # y = 2x\n",
    "params = {'w': 1.5, 'b': 0.1}\n",
    "\n",
    "print(\"Original loss:\", loss_function(params, x_data, y_data))\n",
    "print(\"Gradients:\", grad_loss(params, x_data, y_data))\n",
    "\n",
    "# JIT with vmap for batch processing\n",
    "@jit\n",
    "def batch_prediction(params, x_batch):\n",
    "    \"\"\"Vectorized prediction\"\"\"\n",
    "    return vmap(lambda x: params['w'] * x + params['b'])(x_batch)\n",
    "\n",
    "batch_x = jnp.array([[1., 2.], [3., 4.], [5., 6.]])\n",
    "batch_pred = batch_prediction(params, batch_x)\n",
    "print(f\"Batch predictions: {batch_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With JIT (using jax.debug.print):\n",
      "Debug: intermediate value = [2. 4.]\n",
      "\n",
      "Without JIT (for debugging):\n",
      "Debug: intermediate value = [2. 4.]\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Debugging JIT Functions\n",
    "@jit\n",
    "def debug_function(x):\n",
    "    \"\"\"Function to demonstrate debugging techniques\"\"\"\n",
    "    intermediate = x * 2\n",
    "    # Use jax.debug.print for debugging inside JIT\n",
    "    jax.debug.print(\"Debug: intermediate value = {}\", intermediate)\n",
    "    result = jnp.sin(intermediate)\n",
    "    return result\n",
    "\n",
    "# Temporarily disable JIT for debugging\n",
    "def debug_function_no_jit(x):\n",
    "    \"\"\"Same function without JIT for debugging\"\"\"\n",
    "    intermediate = x * 2\n",
    "    print(f\"Debug: intermediate value = {intermediate}\")  # Regular print works\n",
    "    result = jnp.sin(intermediate)\n",
    "    return result\n",
    "\n",
    "x = jnp.array([1.0, 2.0])\n",
    "print(\"With JIT (using jax.debug.print):\")\n",
    "result_jit = debug_function(x)\n",
    "\n",
    "print(\"\\nWithout JIT (for debugging):\")\n",
    "result_no_jit = debug_function_no_jit(x)\n",
    "\n",
    "print(f\"Results match: {jnp.allclose(result_jit, result_no_jit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Common JIT Pitfalls and Solutions ===\n",
      "\n",
      "1. Side Effects Issue:\n",
      "Counter after two calls: 1\n",
      "\n",
      "2. Python Containers Issue:\n",
      "List approach: [2 4 6]\n",
      "Array approach: [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Common Pitfalls and Best Practices\n",
    "print(\"=== Common JIT Pitfalls and Solutions ===\\n\")\n",
    "\n",
    "# Pitfall 1: Side effects\n",
    "print(\"1. Side Effects Issue:\")\n",
    "counter = 0\n",
    "\n",
    "@jit\n",
    "def bad_side_effect(x):\n",
    "    global counter\n",
    "    counter += 1  # This won't work as expected in JIT!\n",
    "    return x * 2\n",
    "\n",
    "# The counter increment happens only during compilation\n",
    "result1 = bad_side_effect(jnp.array([1.0]))\n",
    "result2 = bad_side_effect(jnp.array([2.0]))  # Same shape, uses cached version\n",
    "print(f\"Counter after two calls: {counter}\")  # Will be 1, not 2!\n",
    "\n",
    "# Solution: Keep state functional\n",
    "def good_functional_approach(x, counter):\n",
    "    return x * 2, counter + 1\n",
    "\n",
    "# Pitfall 2: Using Python containers\n",
    "print(\"\\n2. Python Containers Issue:\")\n",
    "\n",
    "@jit\n",
    "def list_function(x):\n",
    "    # This works but isn't efficient\n",
    "    result_list = []\n",
    "    for i in range(3):\n",
    "        result_list.append(x[i] * 2)\n",
    "    return jnp.array(result_list)\n",
    "\n",
    "# Better approach\n",
    "@jit\n",
    "def array_function(x):\n",
    "    return x[:3] * 2\n",
    "\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "print(f\"List approach: {list_function(x)}\")\n",
    "print(f\"Array approach: {array_function(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
