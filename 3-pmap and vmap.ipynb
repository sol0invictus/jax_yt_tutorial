{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Tutorial: Understanding `vmap` and `pmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `vmap` (Vectorizing Map)\n",
    "\n",
    "`vmap` is a JAX transformation that allows you to automatically vectorize functions. This means you can write a function that operates on a single data point, and `vmap` will transform it into a function that operates efficiently on a batch of data points without you needing to manually add batch dimensions or loops.\n",
    "\n",
    "**Key benefits of `vmap`:**\n",
    "- **Simplicity:** Write code for single examples, and `vmap` handles batching.\n",
    "- **Efficiency:** Leverages JAX's XLA compilation for optimized batch operations.\n",
    "- **Flexibility:** Works with complex functions and PyTrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.6.1\n",
      "JAX backend: cpu\n",
      "Available JAX devices: [CpuDevice(id=0)]\n",
      "Number of local devices: 1\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "\n",
    "# Ensure JAX is using the CPU for consistent behavior in this example notebook\n",
    "# You can change this to 'gpu' or 'tpu' if available and desired.\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"Available JAX devices: {jax.devices()}\")\n",
    "print(f\"Number of local devices: {jax.local_device_count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe58cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar_func(3.0): 11.0\n",
      "\n",
      "Inputs: [1. 2. 3. 4.]\n",
      "Outputs from vmapped function: [ 3.  6. 11. 18.]\n"
     ]
    }
   ],
   "source": [
    "# Example: A simple scalar function\n",
    "def scalar_func(x):\n",
    "  # print(f\"scalar_func called with x: {x}\") # For demonstration of tracing\n",
    "  return x * x + 2\n",
    "\n",
    "# Let's try it with a single value\n",
    "print(f\"scalar_func(3.0): {scalar_func(jnp.array(3.0))}\")\n",
    "\n",
    "# Now, let's vectorize it with vmap\n",
    "batched_func_vmap = jax.vmap(scalar_func)\n",
    "\n",
    "inputs = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "outputs = batched_func_vmap(inputs)\n",
    "\n",
    "print(f\"\\nInputs: {inputs}\")\n",
    "print(f\"Outputs from vmapped function: {outputs}\")\n",
    "# Note: The print inside scalar_func (if uncommented) will only show one traced call with an abstract value for vmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `vmap` with `in_axes` and `out_axes`\n",
    "\n",
    "`in_axes` and `out_axes` arguments in `vmap` give you fine-grained control over how vectorization happens:\n",
    "\n",
    "- **`in_axes`**: Specifies which axes of the input arguments should be mapped over. \n",
    "    - An integer indicates the axis to map over for that argument.\n",
    "    - `None` means the argument is broadcasted (not mapped over).\n",
    "    - It's a tuple/list matching the number of positional arguments.\n",
    "\n",
    "- **`out_axes`**: Specifies where the mapped axis should appear in the output. \n",
    "    - By default, it's `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input xs:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Input y: 3.0\n",
      "Result of vmapped_power_scale (in_axes=(0, None)):\n",
      "[[ 1.  8.]\n",
      " [27. 64.]]\n",
      "Result shape: (2, 2)\n",
      "\n",
      "Batched matrices:\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]]\n",
      "Batched vectors:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Output of batched MVP (in_axes=(0,0)):\n",
      "[[ 5. 11.]\n",
      " [39. 53.]]\n",
      "Output shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Example function with two arguments\n",
    "def power_scale(x, y_scalar):\n",
    "  # x is a vector, y_scalar is a scalar to be broadcasted\n",
    "  # print(f\"power_scale called with x: {x}, y_scalar: {y_scalar}\") # For demonstration\n",
    "  return x ** y_scalar\n",
    "\n",
    "# Map over the first argument (x), broadcast the second (y_scalar)\n",
    "vmapped_power_scale = jax.vmap(power_scale, in_axes=(0, None))\n",
    "\n",
    "xs = jnp.array([[1., 2.], [3., 4.]]) # Batch of 2, each item is a 2-element vector\n",
    "y = jnp.array(3.0)\n",
    "\n",
    "result = vmapped_power_scale(xs, y)\n",
    "print(f\"\\nInput xs:\\n{xs}\")\n",
    "print(f\"Input y: {y}\")\n",
    "print(f\"Result of vmapped_power_scale (in_axes=(0, None)):\\n{result}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Example: Mapping over different axes\n",
    "def matrix_vector_product(matrix, vector):\n",
    "    # matrix is (N, M), vector is (M,)\n",
    "    return jnp.dot(matrix, vector)\n",
    "\n",
    "# Batch of matrices (axis 0), batch of vectors (axis 0)\n",
    "batched_mvp = jax.vmap(matrix_vector_product, in_axes=(0, 0))\n",
    "\n",
    "matrices = jnp.stack([jnp.arange(1,5).reshape(2,2), jnp.arange(5,9).reshape(2,2)])\n",
    "vectors = jnp.stack([jnp.array([1.,2.]), jnp.array([3.,4.])])\n",
    "\n",
    "print(f\"\\nBatched matrices:\\n{matrices}\")\n",
    "print(f\"Batched vectors:\\n{vectors}\")\n",
    "output_mvp = batched_mvp(matrices, vectors)\n",
    "print(f\"Output of batched MVP (in_axes=(0,0)):\\n{output_mvp}\")\n",
    "print(f\"Output shape: {output_mvp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result with default out_axes=0: [11 22 33], shape: (3,)\n",
      "\n",
      "Input (3,2):\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "Output with out_axes=0:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "Shape: (3, 2)\n",
      "Output with out_axes=1:\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "Shape: (2, 3)\n",
      "Output with in_axes=1, out_axes=0:\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Example using out_axes\n",
    "def simple_add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Standard vmap, output batch axis is 0\n",
    "vmapped_add_default_out = jax.vmap(simple_add)\n",
    "xs_add = jnp.array([1, 2, 3])\n",
    "ys_add = jnp.array([10, 20, 30])\n",
    "result_default_out = vmapped_add_default_out(xs_add, ys_add)\n",
    "print(f\"Result with default out_axes=0: {result_default_out}, shape: {result_default_out.shape}\")\n",
    "\n",
    "def identity_func(x):\n",
    "    return x # returns the input as is\n",
    "\n",
    "inputs_2d = jnp.array([[1,2],[3,4],[5,6]]) # Shape (3, 2)\n",
    "\n",
    "# Map over axis 0, place the mapped axis at dimension 0 in output (default)\n",
    "vmapped_identity_out0 = jax.vmap(identity_func, in_axes=0, out_axes=0)\n",
    "output_0 = vmapped_identity_out0(inputs_2d)\n",
    "print(f\"\\nInput (3,2):\\n{inputs_2d}\")\n",
    "print(f\"Output with out_axes=0:\\n{output_0}\\nShape: {output_0.shape}\")\n",
    "\n",
    "# Map over axis 0, place the mapped axis at dimension 1 in output\n",
    "vmapped_identity_out1 = jax.vmap(identity_func, in_axes=0, out_axes=1)\n",
    "output_1 = vmapped_identity_out1(inputs_2d)\n",
    "print(f\"Output with out_axes=1:\\n{output_1}\\nShape: {output_1.shape}\")\n",
    "\n",
    "# Map over axis 1, place the mapped axis at dimension 0 in output\n",
    "vmapped_identity_in1_out0 = jax.vmap(identity_func, in_axes=1, out_axes=0)\n",
    "output_in1_out0 = vmapped_identity_in1_out0(inputs_2d)\n",
    "print(f\"Output with in_axes=1, out_axes=0:\\n{output_in1_out0}\\nShape: {output_in1_out0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e975ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Method 1: Force CPU-only and create fake devices\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"  # This forces JAX to use CPU only\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `pmap` (Parallel Map)\n",
    "\n",
    "`pmap` is JAX's transformation for data parallelism. It compiles a function to run in parallel across multiple devices (e.g., GPUs or TPU cores). This is a form of SPMD (Single Program, Multiple Data) programming.\n",
    "\n",
    "**Key aspects of `pmap`:**\n",
    "- **Device Parallelism:** Executes the same function on different data shards on different devices.\n",
    "- **Data Sharding:** Input data must be explicitly sharded (split) across the first axis to match the number of devices.\n",
    "- **Collective Operations:** Supports operations that communicate across devices (e.g., `jax.lax.psum` for sum-reduction).\n",
    "- **Requires Multiple Devices:** To see true parallelism, you need to run on hardware with multiple JAX-addressable devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JAX local devices: 8\n",
      "\n",
      "Data to shard (shape (8, 4)):\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]\n",
      " [24 25 26 27]\n",
      " [28 29 30 31]]\n",
      "Result from pmapped function (shape (8, 4)):\n",
      "[[ 0  2  4  6]\n",
      " [ 8 10 12 14]\n",
      " [16 18 20 22]\n",
      " [24 26 28 30]\n",
      " [32 34 36 38]\n",
      " [40 42 44 46]\n",
      " [48 50 52 54]\n",
      " [56 58 60 62]]\n",
      "Note: Each device processed its own slice of the input.\n",
      "\n",
      "Data sharded for add_scalar:\n",
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]]\n",
      "Scalar value: 100.0\n",
      "Result from pmapped_add_scalar:\n",
      "[[100. 101.]\n",
      " [102. 103.]\n",
      " [104. 105.]\n",
      " [106. 107.]\n",
      " [108. 109.]\n",
      " [110. 111.]\n",
      " [112. 113.]\n",
      " [114. 115.]]\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available devices\n",
    "num_devices = jax.local_device_count()\n",
    "print(f\"Number of JAX local devices: {num_devices}\")\n",
    "\n",
    "# If only 1 device, pmap will act like a map on that single device.\n",
    "# For true parallelism, you need num_devices > 1.\n",
    "\n",
    "def simple_device_computation(x):\n",
    "  # This function will run on each device with its shard of x\n",
    "  # print(f\"simple_device_computation on device {jax.devices()[jax.process_index()]} with x: {x}\") # This print might be tricky with pmap\n",
    "  return x * 2\n",
    "\n",
    "# Parallelize the function with pmap\n",
    "pmapped_computation = jax.pmap(simple_device_computation)\n",
    "\n",
    "# Create data. The first dimension must be equal to the number of devices.\n",
    "# If num_devices is 1, this will be shape (1, ...)\n",
    "if num_devices > 0:\n",
    "    data_to_shard = jnp.arange(num_devices * 4).reshape((num_devices, 4))\n",
    "    print(f\"\\nData to shard (shape {data_to_shard.shape}):\\n{data_to_shard}\")\n",
    "\n",
    "    # Apply the pmapped function\n",
    "    # Each device gets one row of 'data_to_shard'\n",
    "    result_pmap = pmapped_computation(data_to_shard)\n",
    "    result_pmap.block_until_ready() # Ensure computation finishes for accurate inspection\n",
    "    print(f\"Result from pmapped function (shape {result_pmap.shape}):\\n{result_pmap}\")\n",
    "    print(\"Note: Each device processed its own slice of the input.\")\n",
    "else:\n",
    "    print(\"\\nNo JAX devices found to demonstrate pmap. This usually means JAX isn't properly initialized or no backend is available.\")\n",
    "\n",
    "# Example: pmap with a scalar input (broadcasted)\n",
    "def add_scalar_on_device(x, s):\n",
    "    return x + s\n",
    "\n",
    "# pmap with in_axes=(0, None) to map over x and broadcast s\n",
    "pmapped_add_scalar = jax.pmap(add_scalar_on_device, in_axes=(0, None))\n",
    "\n",
    "if num_devices > 0:\n",
    "    data_sharded = jnp.arange(num_devices * 2).reshape((num_devices, 2))\n",
    "    scalar_val = jnp.array(100.0)\n",
    "    print(f\"\\nData sharded for add_scalar:\\n{data_sharded}\")\n",
    "    print(f\"Scalar value: {scalar_val}\")\n",
    "    result_add_scalar = pmapped_add_scalar(data_sharded, scalar_val)\n",
    "    result_add_scalar.block_until_ready()\n",
    "    print(f\"Result from pmapped_add_scalar:\\n{result_add_scalar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pmap` with `axis_name` for Collective Operations\n",
    "\n",
    "`pmap` can define a named axis using the `axis_name` argument. This name can then be used in collective operations like `jax.lax.psum`, `jax.lax.pmean`, `jax.lax.all_gather`, etc., to communicate or aggregate results across devices participating in the `pmap`.\n",
    "\n",
    "This is fundamental for many distributed algorithms, like training large neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-device values: [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "Total sum (available on all devices): [28. 28. 28. 28. 28. 28. 28. 28.]\n",
      "Expected sum: 28.0\n",
      "\n",
      "Data for global normalization (shape (8, 2)):\n",
      "[[ 1.  2.]\n",
      " [ 3.  4.]\n",
      " [ 5.  6.]\n",
      " [ 7.  8.]\n",
      " [ 9. 10.]\n",
      " [11. 12.]\n",
      " [13. 14.]\n",
      " [15. 16.]]\n",
      "Globally normalized data (shape (8, 2)):\n",
      "[[0.00735294 0.01470588]\n",
      " [0.02205882 0.02941176]\n",
      " [0.03676471 0.04411765]\n",
      " [0.05147059 0.05882353]\n",
      " [0.06617647 0.07352941]\n",
      " [0.08088236 0.0882353 ]\n",
      " [0.09558824 0.10294118]\n",
      " [0.11029412 0.11764706]]\n",
      "Sum of one shard of normalized data: 0.022058824077248573\n",
      "Total sum of all normalized data: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define a function that performs a sum reduction across devices\n",
    "def sum_across_devices(x):\n",
    "  # 'devices' is the named axis for pmap\n",
    "  # jax.lax.psum will sum the value of 'x' from all devices involved in this pmap\n",
    "  return jax.lax.psum(x, axis_name='devices')\n",
    "\n",
    "# pmap this function, naming the mapped axis 'devices'\n",
    "pmapped_sum = jax.pmap(sum_across_devices, axis_name='devices')\n",
    "\n",
    "if num_devices > 0:\n",
    "    # Create data such that each device gets a different scalar value\n",
    "    # For example, if num_devices=4, devices get [0, 1, 2, 3]\n",
    "    per_device_values = jnp.arange(num_devices, dtype=jnp.float32)\n",
    "    print(f\"\\nPer-device values: {per_device_values}\")\n",
    "\n",
    "    # When pmapped_sum is called, each device will have one of these values for 'x'.\n",
    "    # jax.lax.psum will sum them all up, and each device will receive the total sum.\n",
    "    total_sum_on_all_devices = pmapped_sum(per_device_values)\n",
    "    total_sum_on_all_devices.block_until_ready()\n",
    "\n",
    "    print(f\"Total sum (available on all devices): {total_sum_on_all_devices}\")\n",
    "    print(f\"Expected sum: {jnp.sum(per_device_values)}\")\n",
    "\n",
    "    # Example: Normalize data across devices\n",
    "    def normalize_globally(x):\n",
    "        local_sum = jnp.sum(x)\n",
    "        global_sum = jax.lax.psum(local_sum, axis_name='devices')\n",
    "        # Add a small epsilon to prevent division by zero if global_sum is zero\n",
    "        return x / (global_sum + 1e-8)\n",
    "    \n",
    "    pmapped_normalize = jax.pmap(normalize_globally, axis_name='devices')\n",
    "    \n",
    "    # Each device has a small vector\n",
    "    data_for_norm = jnp.arange(num_devices * 2, dtype=jnp.float32).reshape((num_devices, 2)) + 1.0\n",
    "    print(f\"\\nData for global normalization (shape {data_for_norm.shape}):\\n{data_for_norm}\")\n",
    "    \n",
    "    normalized_data = pmapped_normalize(data_for_norm)\n",
    "    normalized_data.block_until_ready()\n",
    "    print(f\"Globally normalized data (shape {normalized_data.shape}):\\n{normalized_data}\")\n",
    "    \n",
    "    # Verify: the sum of all elements in normalized_data should be close to 1.0\n",
    "    # Each device has its part of the normalized data.\n",
    "    # To get the total sum for verification, we'd need to sum it up (conceptually)\n",
    "    if num_devices > 0:\n",
    "      print(f\"Sum of one shard of normalized data: {jnp.sum(normalized_data[0])}\") \n",
    "      # The sum of *all* elements across *all* devices in `normalized_data` will be 1.0.\n",
    "      # We can verify this by summing all parts of normalized_data (which are on the host now)\n",
    "      print(f\"Total sum of all normalized data: {jnp.sum(normalized_data)}\")\n",
    "else:\n",
    "    print(\"\\nSkipping pmap collective examples as num_devices is 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: `vmap` vs. `pmap` vs. Sequential\n",
    "\n",
    "Let's compare the performance of applying a function to a batch of data using:\n",
    "1.  A standard Python loop applying a JITted JAX function to each item.\n",
    "2.  `jax.vmap` to vectorize the function.\n",
    "3.  `jax.pmap` to parallelize the function across available devices.\n",
    "\n",
    "We'll use a slightly more compute-intensive function for this benchmark. For accurate timing in JAX, especially with asynchronous dispatch, it's crucial to call `.block_until_ready()` on the results before stopping the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated batched_matrices with shape: (64, 256, 256)\n",
      "\n",
      "Timing sequential execution...\n",
      "Sequential execution time: 0.2050 seconds\n",
      "\n",
      "Timing vmap execution...\n",
      "vmap execution time: 0.1385 seconds\n",
      "\n",
      "Timing pmap execution...\n",
      "Reshaped matrices for pmap to shape: (8, 8, 256, 256)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "matrix_size = 256 # Size of square matrices for the benchmark task\n",
    "batch_size = 64   # Number of matrices to process\n",
    "\n",
    "# Define a JITted function for our benchmark task\n",
    "@jax.jit\n",
    "def benchmark_task(matrix):\n",
    "    # Perform some operations that are reasonably compute-intensive\n",
    "    res = matrix\n",
    "    for _ in range(3):\n",
    "        res = jnp.dot(res, matrix) # Matrix multiplication\n",
    "        res = jnp.tanh(res)       # Element-wise non-linearity\n",
    "    return jnp.sum(res) # Return a scalar summary\n",
    "\n",
    "# Generate a batch of random matrices\n",
    "key, *subkeys = jax.random.split(key, batch_size + 1)\n",
    "batched_matrices = jnp.stack([jax.random.normal(subkeys[i], (matrix_size, matrix_size)) for i in range(batch_size)])\n",
    "print(f\"Generated batched_matrices with shape: {batched_matrices.shape}\")\n",
    "\n",
    "# --- 1. Sequential execution (Python loop over JITted function) ---\n",
    "print(\"\\nTiming sequential execution...\")\n",
    "start_time_seq = time.time()\n",
    "results_seq = []\n",
    "for i in range(batch_size):\n",
    "    results_seq.append(benchmark_task(batched_matrices[i]))\n",
    "# Stack results and block for timing\n",
    "jnp.stack(results_seq).block_until_ready()\n",
    "end_time_seq = time.time()\n",
    "time_seq = end_time_seq - start_time_seq\n",
    "print(f\"Sequential execution time: {time_seq:.4f} seconds\")\n",
    "\n",
    "# --- 2. `vmap` execution ---\n",
    "print(\"\\nTiming vmap execution...\")\n",
    "vmapped_task = jax.vmap(benchmark_task)\n",
    "start_time_vmap = time.time()\n",
    "results_vmap = vmapped_task(batched_matrices)\n",
    "results_vmap.block_until_ready() # Important for accurate timing\n",
    "end_time_vmap = time.time()\n",
    "time_vmap = end_time_vmap - start_time_vmap\n",
    "print(f\"vmap execution time: {time_vmap:.4f} seconds\")\n",
    "\n",
    "# --- 3. `pmap` execution ---\n",
    "print(\"\\nTiming pmap execution...\")\n",
    "if num_devices > 0:\n",
    "    # pmap requires the leading dimension to be equal to num_devices\n",
    "    # We need to ensure batch_size is a multiple of num_devices for this simple setup\n",
    "    if batch_size % num_devices == 0:\n",
    "        sharded_batch_size = batch_size // num_devices\n",
    "        sharded_matrices = batched_matrices.reshape((num_devices, sharded_batch_size, matrix_size, matrix_size))\n",
    "        print(f\"Reshaped matrices for pmap to shape: {sharded_matrices.shape}\")\n",
    "\n",
    "        pmapped_task = jax.pmap(benchmark_task)\n",
    "        \n",
    "        # Warm-up pmap (compilation can take time on first run)\n",
    "        warmup_result = pmapped_task(sharded_matrices)\n",
    "        warmup_result.block_until_ready()\n",
    "\n",
    "        start_time_pmap = time.time()\n",
    "        results_pmap = pmapped_task(sharded_matrices)\n",
    "        results_pmap.block_until_ready() # Important for accurate timing\n",
    "        end_time_pmap = time.time()\n",
    "        time_pmap = end_time_pmap - start_time_pmap\n",
    "        print(f\"pmap execution time: {time_pmap:.4f} seconds (on {num_devices} devices)\")\n",
    "        # results_pmap will have shape (num_devices, sharded_batch_size)\n",
    "        # print(f\"pmap result shape: {results_pmap.shape}\")\n",
    "    else:\n",
    "        print(f\"Skipping pmap timing: batch_size ({batch_size}) is not divisible by num_devices ({num_devices}).\")\n",
    "        time_pmap = float('inf') # Or some indicator that it wasn't run\n",
    "else:\n",
    "    print(\"Skipping pmap timing: No JAX devices found or num_devices is 0.\")\n",
    "    time_pmap = float('inf')\n",
    "\n",
    "print(\"\\n--- Timing Summary ---\")\n",
    "print(f\"Sequential: {time_seq:.4f} s\")\n",
    "print(f\"vmap:       {time_vmap:.4f} s\")\n",
    "if time_pmap != float('inf'):\n",
    "    print(f\"pmap:       {time_pmap:.4f} s (on {num_devices} devices)\")\n",
    "    if time_vmap > 0 : print(f\"vmap is {time_seq/time_vmap:.2f}x faster than sequential\")\n",
    "    if time_pmap > 0 and time_vmap > 0 : print(f\"pmap is {time_vmap/time_pmap:.2f}x faster than vmap (approx, highly dependent on setup)\")\n",
    "else:\n",
    "    print(\"pmap:       Not run or N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Timing Results\n",
    "\n",
    "You should typically observe the following:\n",
    "- **`vmap` vs. Sequential:** `vmap` is generally significantly faster than a Python loop calling a JITted function for each item. This is because `vmap` allows JAX to see the entire batch operation at once, enabling XLA to perform aggressive optimizations and parallelize computations on the available hardware (even on a single CPU or GPU core through SIMD instructions).\n",
    "- **`pmap` vs. `vmap`:**\n",
    "    - **On a single device (or if `num_devices` is 1):** `pmap` might be slightly slower than `vmap` due to a small overhead. It essentially behaves like `map` in this scenario.\n",
    "    - **On multiple devices (e.g., multiple GPUs or TPU cores):** `pmap` can be faster than `vmap` if the workload is large enough to benefit from distribution and the communication overhead is managed. The speedup depends on the task, data size, and the efficiency of inter-device communication.\n",
    "    - **On CPUs:** JAX can treat multiple CPU cores as separate devices. `pmap` might show some speedup over `vmap` if the task is parallelizable and the overhead of managing multiple processes/threads for `pmap` is less than the gains from parallelism. However, for many CPU-bound tasks that are already well-optimized by `vmap`, `pmap` might not offer substantial additional benefits and could even add overhead.\n",
    "\n",
    "**Important Considerations:**\n",
    "- **Compilation Time:** The first run of a JITted function (including those transformed by `vmap` or `pmap`) includes compilation time. For fair benchmarking, either perform a warm-up run or use tools like `%timeit` (in a notebook) which handle this.\n",
    "- **Workload Size:** The benefits of `vmap` and `pmap` are more pronounced for larger batches and more computationally intensive tasks.\n",
    "- **Data Sharding for `pmap`:** `pmap` requires data to be explicitly sharded. The leading axis of the input arrays must match the number of devices. This might require reshaping your data.\n",
    "- **Hardware:** The actual performance gains from `pmap` are highly dependent on the underlying hardware (number and type of GPUs/TPUs) and the nature of the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences: `vmap` vs. `pmap`\n",
    "\n",
    "| Feature          | `vmap`                                       | `pmap`                                            |\n",
    "|------------------|----------------------------------------------|---------------------------------------------------|\n",
    "| **Purpose** | Automatic vectorization (batching)         | Parallel execution across devices                 |\n",
    "| **Execution** | Typically on a single device (conceptually)  | Across multiple devices (SPMD)                    |\n",
    "| **Data Handling**| Operates on an axis of an array              | Requires data to be sharded across devices        |\n",
    "| **Communication**| No inherent inter-batch communication        | Supports collective operations (e.g., `psum`)     |\n",
    "| **Use Case** | Applying an operation to a batch of data efficiently | Scaling computation to multiple GPUs/TPUs         |\n",
    "| **`in_axes`** | Flexible control over which args are mapped | Specifies which args are sharded (or broadcasted) |\n",
    "| **`axis_name`** | Not applicable                               | Used to name the mapped axis for collectives      |\n",
    "\n",
    "**When to use which?**\n",
    "- Use **`vmap`** when you have a function written for a single example and want to apply it to a batch of examples efficiently on one device. It's great for adding batch dimensions implicitly.\n",
    "- Use **`pmap`** when you want to distribute the computation of a function across multiple physical devices (like multiple GPUs or TPU cores) to speed up processing or handle larger datasets that don't fit on a single device. This requires thinking about data sharding and potentially inter-device communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Further Resources\n",
    "\n",
    "`vmap` and `pmap` are powerful tools in the JAX ecosystem for writing clean, efficient, and scalable code.\n",
    "- `vmap` simplifies batching.\n",
    "- `pmap` enables multi-device parallelism.\n",
    "\n",
    "Understanding how and when to use them is key to leveraging JAX effectively for high-performance numerical computing and machine learning.\n",
    "\n",
    "**Further Reading:**\n",
    "- JAX Documentation on `vmap`: [https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html)\n",
    "- JAX Documentation on `pmap`: [https://jax.readthedocs.io/en/latest/jax-101/04-parallelization.html](https://jax.readthedocs.io/en/latest/jax-101/04-parallelization.html)\n",
    "- JAX Sharp Bits - Common Gotchas: [https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) (often covers aspects of transformations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
