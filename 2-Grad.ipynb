{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Autograd Tutorial - Video 2\n",
    "## Automatic Differentiation in JAX\n",
    "\n",
    "Welcome to the second video in our JAX series! Today we'll explore automatic differentiation with `jax.grad`, `jax.value_and_grad`, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.6.1\n",
      "Let's explore automatic differentiation in JAX!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad\n",
    "import numpy as np\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(\"Let's explore automatic differentiation in JAX!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = x^2\n",
      "f(3.0) = 9.0\n",
      "f'(3.0) = 6.0\n",
      "Expected: 2 * 3.0 = 6.0\n"
     ]
    }
   ],
   "source": [
    "#Simple scalar function - our first gradient\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Get the gradient function\n",
    "df_dx = grad(f)\n",
    "\n",
    "# Evaluate at x = 3.0\n",
    "x = 3.0\n",
    "gradient_value = df_dx(x)\n",
    "print(f\"f(x) = x^2\")\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {gradient_value}\")\n",
    "print(f\"Expected: 2 * {x} = {2 * x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h(x,y) = x² + 3xy + y²\n",
      "∂h/∂x = 2x + 3y = 7.0\n",
      "∂h/∂y = 3x + 2y = 8.0\n"
     ]
    }
   ],
   "source": [
    "#Function with multiple inputs - partial derivatives\n",
    "def h(x, y):\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# grad() by default takes derivative w.r.t. first argument\n",
    "dh_dx = grad(h, argnums=0)  # ∂h/∂x\n",
    "dh_dy = grad(h, argnums=1)  # ∂h/∂y\n",
    "\n",
    "x, y = 2.0, 1.0\n",
    "print(f\"h(x,y) = x² + 3xy + y²\")\n",
    "print(f\"∂h/∂x = 2x + 3y = {dh_dx(x, y)}\")\n",
    "print(f\"∂h/∂y = 3x + 2y = {dh_dy(x, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate calls:\n",
      "  Value: 11.0\n",
      "  Grad: (Array(7., dtype=float32, weak_type=True), Array(8., dtype=float32, weak_type=True))\n",
      "value_and_grad:\n",
      "  Value: 11.0\n",
      "  Grad: (Array(7., dtype=float32, weak_type=True), Array(8., dtype=float32, weak_type=True))\n",
      "\n",
      "Same results, but value_and_grad is more efficient!\n"
     ]
    }
   ],
   "source": [
    "# value_and_grad - getting both function value and gradient\n",
    "# Often we need both the function value and its gradient\n",
    "def h(x, y):\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# Instead of calling function + grad separately:\n",
    "grad_h = grad(h, argnums=(0, 1))\n",
    "x, y = 2.0, 1.0\n",
    "value_separate = h(x, y)\n",
    "grad_separate = grad_h(x, y)\n",
    "\n",
    "# Use value_and_grad for efficiency:\n",
    "value_and_grad_h = value_and_grad(h, argnums=(0, 1))\n",
    "value_together, grad_together = value_and_grad_h(x, y)\n",
    "\n",
    "print(f\"Separate calls:\")\n",
    "print(f\"  Value: {value_separate}\")\n",
    "print(f\"  Grad: {grad_separate}\")\n",
    "print(f\"value_and_grad:\")\n",
    "print(f\"  Value: {value_together}\")\n",
    "print(f\"  Grad: {grad_together}\")\n",
    "print(f\"\\nSame results, but value_and_grad is more efficient!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parameters: w=1.0, b=0.0\n",
      "Loss: 7.4100\n",
      "Gradients: dL/dw=-14.9000, dL/db=-5.0000\n",
      "\n",
      "This is exactly what you need for gradient descent!\n"
     ]
    }
   ],
   "source": [
    "# value_and_grad in machine learning context\n",
    "def loss_function(params, x_data, y_data):\n",
    "    \"\"\"Simple quadratic loss function\"\"\"\n",
    "    w, b = params\n",
    "    predictions = w * x_data + b\n",
    "    return jnp.mean((predictions - y_data)**2)\n",
    "\n",
    "# Sample data\n",
    "x_data = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "y_data = jnp.array([2.1, 3.9, 6.1, 7.9])  # Noisy y = 2x\n",
    "params = (1.0, 0.0)  # Initial guess: w=1, b=0\n",
    "\n",
    "# Get both loss and gradients in one call\n",
    "loss_and_grad = value_and_grad(loss_function)\n",
    "current_loss, gradients = loss_and_grad(params, x_data, y_data)\n",
    "\n",
    "print(f\"Current parameters: w={params[0]}, b={params[1]}\")\n",
    "print(f\"Loss: {current_loss:.4f}\")\n",
    "print(f\"Gradients: dL/dw={gradients[0]:.4f}, dL/db={gradients[1]:.4f}\")\n",
    "print(f\"\\nThis is exactly what you need for gradient descent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix W:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Function value: 30.0\n",
      "Gradient matrix:\n",
      "[[2. 4.]\n",
      " [6. 8.]]\n",
      "\n",
      "Gradient has the same shape as input!\n"
     ]
    }
   ],
   "source": [
    "# Matrix operations and gradients\n",
    "def matrix_function(W):\n",
    "    # Simple quadratic form: trace(W @ W.T)\n",
    "    return jnp.trace(W @ W.T)\n",
    "\n",
    "# Use value_and_grad with matrices\n",
    "val_grad_matrix = value_and_grad(matrix_function)\n",
    "\n",
    "W = jnp.array([[1.0, 2.0], \n",
    "               [3.0, 4.0]])\n",
    "value, gradient_W = val_grad_matrix(W)\n",
    "print(f\"Input matrix W:\\n{W}\")\n",
    "print(f\"Function value: {value}\")\n",
    "print(f\"Gradient matrix:\\n{gradient_W}\")\n",
    "print(f\"\\nGradient has the same shape as input!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = x⁴ - 2x³ + x²\n",
      "At x=2.0:\n",
      "f(2.0) = 4.0\n",
      "f'(2.0) = 12.0\n",
      "f''(2.0) = 26.0\n",
      "\n",
      "Expected: f'(x) = 4x³ - 6x² + 2x = 12.0\n"
     ]
    }
   ],
   "source": [
    "# Higher-order derivatives with value_and_grad\n",
    "def f(x):\n",
    "    return x**4 - 2*x**3 + x**2\n",
    "\n",
    "# First derivative with value\n",
    "f_val_grad = value_and_grad(f)\n",
    "# Second derivative\n",
    "f_prime = grad(f)\n",
    "f_double_prime = grad(f_prime)\n",
    "\n",
    "x = 2.0\n",
    "value, first_deriv = f_val_grad(x)\n",
    "second_deriv = f_double_prime(x)\n",
    "\n",
    "print(f\"f(x) = x⁴ - 2x³ + x²\")\n",
    "print(f\"At x={x}:\")\n",
    "print(f\"f({x}) = {value}\")\n",
    "print(f\"f'({x}) = {first_deriv}\")\n",
    "print(f\"f''({x}) = {second_deriv}\")\n",
    "print(f\"\\nExpected: f'(x) = 4x³ - 6x² + 2x = {4*x**3 - 6*x**2 + 2*x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent with value_and_grad:\n",
      "Step 1: w=2.000, b=0.650, loss=0.8750\n",
      "Step 2: w=1.675, b=0.520, loss=0.4225\n",
      "Step 3: w=1.903, b=0.578, loss=0.2176\n",
      "Step 4: w=1.760, b=0.512, loss=0.1239\n",
      "Step 5: w=1.864, b=0.529, loss=0.0803\n",
      "\n",
      "Target: w=2.0, b=0.0\n"
     ]
    }
   ],
   "source": [
    "# Complete gradient descent with value_and_grad\n",
    "def mse_loss(params, x_data, y_data):\n",
    "    \"\"\"Mean squared error for linear regression\"\"\"\n",
    "    w, b = params\n",
    "    predictions = w * x_data + b\n",
    "    return jnp.mean((predictions - y_data)**2)\n",
    "\n",
    "# Create the loss and gradient function\n",
    "loss_and_grad = value_and_grad(mse_loss)\n",
    "\n",
    "def gradient_descent_step(params, x_data, y_data, learning_rate=0.1):\n",
    "    \"\"\"One step of gradient descent using value_and_grad\"\"\"\n",
    "    loss, grads = loss_and_grad(params, x_data, y_data)\n",
    "    # Update parameters: θ = θ - α∇L\n",
    "    new_w = params[0] - learning_rate * grads[0]\n",
    "    new_b = params[1] - learning_rate * grads[1]\n",
    "    return (new_w, new_b), loss\n",
    "\n",
    "# Sample data\n",
    "x_data = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "y_data = jnp.array([2.0, 4.0, 6.0, 8.0])  # y = 2x\n",
    "\n",
    "# Run gradient descent\n",
    "params = (1.5, 0.5)\n",
    "print(\"Gradient descent with value_and_grad:\")\n",
    "for i in range(5):\n",
    "    params, loss = gradient_descent_step(params, x_data, y_data)\n",
    "    print(f\"Step {i+1}: w={params[0]:.3f}, b={params[1]:.3f}, loss={loss:.4f}\")\n",
    "\n",
    "print(f\"\\nTarget: w=2.0, b=0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
